{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNVfovZk+lEy2vlTd4acQoJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Import packages"],"metadata":{"id":"3LxcVFAtCcaF"}},{"cell_type":"code","source":["from huggingface_hub import create_repo, HfApi\n","from google.colab import userdata\n","import os"],"metadata":{"id":"0LLCdaYopnXe","executionInfo":{"status":"ok","timestamp":1722603598273,"user_tz":-120,"elapsed":879,"user":{"displayName":"anass majji","userId":"00589836281073803991"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["# Main code"],"metadata":{"id":"7n_B7HVyCnwg"}},{"cell_type":"code","source":["# Variables\n","MODEL_ID = \"model_to_quantize\"\n","QUANTIZATION_METHODS = [\"q4_k_m\", \"q5_k_m\"]\n","\n","# Constants\n","MODEL_NAME = MODEL_ID.split('/')[-1]"],"metadata":{"id":"C5n79c_eHGBw","executionInfo":{"status":"ok","timestamp":1722603601112,"user_tz":-120,"elapsed":234,"user":{"displayName":"anass majji","userId":"00589836281073803991"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Install llama.cpp\n","!git clone https://github.com/ggerganov/llama.cpp\n","!cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make\n","!pip install -r llama.cpp/requirements.txt\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G5jvXz-UHHEP","executionInfo":{"status":"ok","timestamp":1722603678273,"user_tz":-120,"elapsed":76791,"user":{"displayName":"anass majji","userId":"00589836281073803991"}},"outputId":"5c1fccb2-15ed-4fb2-917b-b09d85802977"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'llama.cpp'...\n","remote: Enumerating objects: 30876, done.\u001b[K\n","remote: Counting objects: 100% (6127/6127), done.\u001b[K\n","remote: Compressing objects: 100% (214/214), done.\u001b[K\n","remote: Total 30876 (delta 6030), reused 5923 (delta 5913), pack-reused 24749\u001b[K\n","Receiving objects: 100% (30876/30876), 53.46 MiB | 19.15 MiB/s, done.\n","Resolving deltas: 100% (22132/22132), done.\n","Already up to date.\n","I ccache not found. Consider installing it for faster compilation.\n","I llama.cpp build info: \n","I UNAME_S:   Linux\n","I UNAME_P:   x86_64\n","I UNAME_M:   x86_64\n","I CFLAGS:    -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion \n","I CXXFLAGS:  -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE \n","I NVCCFLAGS: -std=c++11 -O3 -g \n","I LDFLAGS:    \n","I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n","I CXX:       c++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n","\n","rm -vrf *.dot libllava.a llama-baby-llama llama-batched llama-batched-bench llama-bench llama-benchmark-matmult llama-cli llama-convert-llama2c-to-ggml llama-embedding llama-eval-callback llama-export-lora llama-gbnf-validator llama-gguf llama-gguf-hash llama-gguf-split llama-gritlm llama-imatrix llama-infill llama-llava-cli llama-lookahead llama-lookup llama-lookup-create llama-lookup-merge llama-lookup-stats llama-parallel llama-passkey llama-perplexity llama-q8dot llama-quantize llama-quantize-stats llama-retrieval llama-save-load-state llama-server llama-simple llama-speculative llama-tokenize llama-vdot llama-cvector-generator tests/test-c.o tests/test-autorelease tests/test-backend-ops tests/test-chat-template tests/test-double-float tests/test-grad0 tests/test-grammar-integration tests/test-grammar-parser tests/test-json-schema-to-grammar tests/test-llama-grammar tests/test-model-load-cancel tests/test-opt tests/test-quantize-fns tests/test-quantize-perf tests/test-rope tests/test-sampling tests/test-tokenizer-0 tests/test-tokenizer-1-bpe tests/test-tokenizer-1-spm\n","rm -rvf src/*.o\n","rm -rvf tests/*.o\n","rm -rvf examples/*.o\n","rm -rvf common/*.o\n","rm -rvf *.a\n","rm -rvf *.dll\n","rm -rvf *.so\n","rm -rvf *.dot\n","rm -rvf ggml/*.a\n","rm -rvf ggml/*.dll\n","rm -rvf ggml/*.so\n","rm -vrf ggml/src/*.o\n","rm -rvf common/build-info.cpp\n","rm -vrf ggml/src/ggml-metal-embed.metal\n","rm -vrf ggml/src/ggml-cuda/*.o\n","rm -vrf ggml/src/ggml-cuda/template-instances/*.o\n","rm -rvf libllava.a llama-baby-llama llama-batched llama-batched-bench llama-bench llama-benchmark-matmult llama-cli llama-convert-llama2c-to-ggml llama-embedding llama-eval-callback llama-export-lora llama-gbnf-validator llama-gguf llama-gguf-hash llama-gguf-split llama-gritlm llama-imatrix llama-infill llama-llava-cli llama-lookahead llama-lookup llama-lookup-create llama-lookup-merge llama-lookup-stats llama-parallel llama-passkey llama-perplexity llama-q8dot llama-quantize llama-quantize-stats llama-retrieval llama-save-load-state llama-server llama-simple llama-speculative llama-tokenize llama-vdot llama-cvector-generator tests/test-c.o\n","rm -rvf tests/test-autorelease tests/test-backend-ops tests/test-chat-template tests/test-double-float tests/test-grad0 tests/test-grammar-integration tests/test-grammar-parser tests/test-json-schema-to-grammar tests/test-llama-grammar tests/test-model-load-cancel tests/test-opt tests/test-quantize-fns tests/test-quantize-perf tests/test-rope tests/test-sampling tests/test-tokenizer-0 tests/test-tokenizer-1-bpe tests/test-tokenizer-1-spm\n","rm -f vulkan-shaders-gen ggml/src/ggml-vulkan-shaders.hpp ggml/src/ggml-vulkan-shaders.cpp\n","rm -rvf main quantize quantize-stats perplexity imatrix embedding vdot q8dot convert-llama2c-to-ggml simple batched batched-bench save-load-state server gguf gguf-split eval-callback llama-bench libllava.a llava-cli baby-llama retrieval speculative infill tokenize benchmark-matmult parallel export-lora lookahead lookup passkey gritlm\n","find examples pocs -type f -name \"*.o\" -delete\n","Makefile:75: *** LLAMA_CUBLAS is removed. Use GGML_CUDA instead..  Stop.\n","Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu\n","Requirement already satisfied: numpy~=1.26.4 in /usr/local/lib/python3.10/dist-packages (from -r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 1)) (1.26.4)\n","Collecting sentencepiece~=0.2.0 (from -r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 2))\n","  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n","Requirement already satisfied: transformers<5.0.0,>=4.40.1 in /usr/local/lib/python3.10/dist-packages (from -r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.42.4)\n","Collecting gguf>=0.1.0 (from -r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 4))\n","  Downloading gguf-0.9.1-py3-none-any.whl.metadata (3.3 kB)\n","Collecting protobuf<5.0.0,>=4.21.0 (from -r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 5))\n","  Downloading protobuf-4.25.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n","Collecting torch~=2.2.1 (from -r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3))\n","  Downloading https://download.pytorch.org/whl/cpu/torch-2.2.2%2Bcpu-cp310-cp310-linux_x86_64.whl (186.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.8/186.8 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.40.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.15.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.40.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.40.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.40.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.40.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.40.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2.31.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.40.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.4.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.40.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.40.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.66.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2024.6.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch~=2.2.1->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.40.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.40.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.40.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.40.1->-r llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.7.4)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch~=2.2.1->-r llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.3.0)\n","Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gguf-0.9.1-py3-none-any.whl (49 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading protobuf-4.25.4-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece, protobuf, gguf, torch\n","  Attempting uninstall: sentencepiece\n","    Found existing installation: sentencepiece 0.1.99\n","    Uninstalling sentencepiece-0.1.99:\n","      Successfully uninstalled sentencepiece-0.1.99\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.20.3\n","    Uninstalling protobuf-3.20.3:\n","      Successfully uninstalled protobuf-3.20.3\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.3.1+cu121\n","    Uninstalling torch-2.3.1+cu121:\n","      Successfully uninstalled torch-2.3.1+cu121\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.4 which is incompatible.\n","torchaudio 2.3.1+cu121 requires torch==2.3.1, but you have torch 2.2.2+cpu which is incompatible.\n","torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.2.2+cpu which is incompatible.\n","torchvision 0.18.1+cu121 requires torch==2.3.1, but you have torch 2.2.2+cpu which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gguf-0.9.1 protobuf-4.25.4 sentencepiece-0.2.0 torch-2.2.2+cpu\n"]}]},{"cell_type":"code","source":["# Download model\n","!git lfs install\n","!git clone https://huggingface.co/{MODEL_ID}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G7rbj144HIKG","executionInfo":{"status":"ok","timestamp":1722604639258,"user_tz":-120,"elapsed":960367,"user":{"displayName":"anass majji","userId":"00589836281073803991"}},"outputId":"4d08a51f-ae91-4f84-ba95-a5cf3586bb41"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Git LFS initialized.\n","Cloning into 'EvolCodeLlama-7b'...\n","remote: Enumerating objects: 35, done.\u001b[K\n","remote: Total 35 (delta 0), reused 0 (delta 0), pack-reused 35 (from 1)\u001b[K\n","Unpacking objects: 100% (35/35), 483.38 KiB | 2.75 MiB/s, done.\n","Filtering content: 100% (5/5), 4.70 GiB | 5.02 MiB/s, done.\n","Encountered 1 file(s) that may not have been copied correctly on Windows:\n","\tpytorch_model-00001-of-00002.bin\n","\n","See: `git lfs help smudge` for more details.\n"]}]},{"cell_type":"code","source":["# Convert to fp16\n","fp16 = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.fp16.bin\"\n","!python llama.cpp/convert_hf_to_gguf_update.py {MODEL_NAME} --outtype f16 --outfile {fp16}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yUhNPdt-HJbH","executionInfo":{"status":"ok","timestamp":1722605916339,"user_tz":-120,"elapsed":6283,"user":{"displayName":"anass majji","userId":"00589836281073803991"}},"outputId":"0d9688c1-118d-4a2b-ee99-6a4f954fdbac"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["INFO:convert_hf_to_gguf_update:Usage: python convert_hf_to_gguf_update.py <huggingface_token>\n"]}]},{"cell_type":"code","source":["# Quantize the model for each method in the QUANTIZATION_METHODS list\n","for method in QUANTIZATION_METHODS:\n","    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n","    !./llama.cpp/quantize {fp16} {qtype} {method}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rKPFjOmmHKk-","executionInfo":{"status":"ok","timestamp":1722605933043,"user_tz":-120,"elapsed":371,"user":{"displayName":"anass majji","userId":"00589836281073803991"}},"outputId":"6f5f44d5-d76b-4788-cb3b-e71aaed74853"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: line 1: ./llama.cpp/quantize: No such file or directory\n","/bin/bash: line 1: ./llama.cpp/quantize: No such file or directory\n"]}]},{"cell_type":"code","source":["model_list = [file for file in os.listdir(MODEL_NAME) if \"gguf\" in file]\n","\n","prompt = input(\"Enter your prompt: \")\n","chosen_method = input(\"Name of the model (options: \" + \", \".join(model_list) + \"): \")\n","\n","# Verify the chosen method is in the list\n","if chosen_method not in model_list:\n","    print(\"Invalid name\")\n","else:\n","    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n","    !./llama.cpp/main -m {qtype} -n 128 --color -ngl 35 -p \"{prompt}\""],"metadata":{"id":"grLdKbZbHWyq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Push to Hugging Face"],"metadata":{"id":"kAmBtrjZHklp"}},{"cell_type":"code","source":["# install huggingface_hub\n","!pip install -q huggingface_hub\n"],"metadata":{"id":"X3GdO_pKHtJu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["username = \"username\"\n","\n","# Defined in the secrets tab in Google Colab\n","api = HfApi(token=userdata.get(\"hf_secret_key\"))\n","\n","# Create empty repo\n","create_repo(\n","    repo_id = f\"{username}/{MODEL_NAME}-GGUF\",\n","    repo_type=\"model\",\n","    exist_ok=True,\n",")\n","\n","# Upload gguf files\n","api.upload_folder(\n","    folder_path=MODEL_NAME,\n","    repo_id=f\"{username}/{MODEL_NAME}-GGUF\",\n","    allow_patterns=f\"*.gguf\",\n",")"],"metadata":{"id":"HWV33mkVHW03"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0g__JELgYSim"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3Yu924ywaGD8"},"execution_count":null,"outputs":[]}]}